# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124FHbbhekrT7CRXS8hxxew58yF5clF75

# **Audio**
"""

from google.colab import drive
drive.mount("/content/drive", force_remount=True)

import numpy as np
import librosa
import librosa.display
import matplotlib.pyplot as plt
import json
import os
import math
from sklearn.model_selection import train_test_split
import tensorflow.keras as keras
import random

DATASET_PATH = "/content/drive/MyDrive/Capstone/Audio-modality/dataset"
JSON_PATH = "/content/drive/MyDrive/Capstone/Audio-modality/data_10.json"
SAMPLE_RATE = 22050
TRACK_DURATION = 30 # measured in seconds
SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION

def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):
    """Extracts MFCCs from music dataset and saves them into a json file along with genre labels.
        """

    # dictionary to store mapping, labels, and MFCCs
    data = {
        "mapping": [],
        "labels": [],
        "mfcc": []
    }

    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)
    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)

    # loop through all genre sub-folder
    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):

        # ensure we're processing a genre sub-folder level
        if dirpath is not dataset_path:

            # save genre label (i.e., sub-folder name) in the mapping
            semantic_label = dirpath.split("/")[-1]
            data["mapping"].append(semantic_label)
            print("\nProcessing: {}".format(semantic_label))

            # process all audio files in genre sub-dir
            for f in filenames:

		# load audio file
                file_path = os.path.join(dirpath, f)
                signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)

                # process all segments of audio file
                for d in range(num_segments):

                    # calculate start and finish sample for current segment
                    start = samples_per_segment * d
                    finish = start + samples_per_segment

                    # extract mfcc
                    mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length, dct_type=3)
                    mfcc = mfcc.T

                    # store only mfcc feature with expected number of vectors
                    if len(mfcc) == num_mfcc_vectors_per_segment:
                        data["mfcc"].append(mfcc.tolist())
                        data["labels"].append(i-1)
                        print("{}, segment:{}".format(file_path, d+1))

    # save MFCCs to json file
    with open(json_path, "w") as fp:
        json.dump(data, fp, indent=4)

save_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)

DATA_PATH = "/content/drive/MyDrive/Capstone/Audio-modality/data_10.json"

def load_data(data_path):

    with open(data_path, "r") as f:
        data = json.load(f)

    # convert lists to numpy arrays
    X = np.array(data["mfcc"])
    y = np.array(data["labels"])

    print("Data succesfully loaded!")

    return  X, y

# load data
X, y = load_data(DATA_PATH)

X.shape

# create train/test split

# create train, validation and test split
audioX_train,audioX_test, audioy_train, audioy_test = train_test_split(X, y, test_size=0.25)
audioX_train, audioX_validation, audioy_train, audioy_validation = train_test_split(audioX_train, audioy_train, test_size=0.2)

# add an axis to input sets
audioX_train = audioX_train[..., np.newaxis]
audioX_validation = audioX_validation[..., np.newaxis]
audioX_test = audioX_test[..., np.newaxis]

audioX_train.shape

input_shape = (audioX_train.shape[1], audioX_train.shape[2], 1)

# build the CNN
model_cnn = keras.Sequential()
# model_cnn.add(data_augmentation)
# 1st conv layer
model_cnn.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))
model_cnn.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
model_cnn.add(keras.layers.BatchNormalization())

# 2nd conv layer
model_cnn.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))
model_cnn.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
model_cnn.add(keras.layers.BatchNormalization())

# 3rd conv layer
model_cnn.add(keras.layers.Conv2D(32, (2, 2), activation='relu'))
model_cnn.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))
model_cnn.add(keras.layers.BatchNormalization())

# flatten output and feed it into dense layer
model_cnn.add(keras.layers.Flatten())
# model_cnn.add(keras.layers.Dense(256, activation='relu'))
# model_cnn.add(keras.layers.Dropout(0.3))
model_cnn.add(keras.layers.Dense(64, activation='relu'))
model_cnn.add(keras.layers.Dropout(0.3))

# output layer
model_cnn.add(keras.layers.Dense(10, activation='softmax'))

# compile model
optimiser = keras.optimizers.Adam(learning_rate=0.0001)
model_cnn.compile(optimizer=optimiser,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model_cnn.summary()

# train model
history = model_cnn.fit(audioX_train, audioy_train, validation_data=(audioX_validation, audioy_validation), batch_size=32, epochs=28)

# plot accuracy and error as a function of the epochs
#plt.history(history)
fig = plt.figure()
plt.plot(history.history['accuracy'], color='teal', label='accuracy')
plt.plot(history.history['val_accuracy'], color='orange', label='val_accuracy')
fig.suptitle('Accuracy', fontsize=20)
plt.legend(loc="upper left")
plt.show()

fig = plt.figure()
plt.plot(history.history['loss'], color='teal', label='loss')
plt.plot(history.history['val_loss'], color='orange', label='val_loss')
fig.suptitle('Loss', fontsize=20)
plt.legend(loc="upper left")
plt.show()

# evaluate model on Test Set
test_loss, test_acc = model_cnn.evaluate(audioX_test, audioy_test, verbose=2)
print('\nTest accuracy:', test_acc)

#model_cnn.save("Music_Genre_10_CNN.h5")
model_cnn.save("Audio.hdf5")

from sklearn.metrics import confusion_matrix

"""prediction on test set"""

# pick a sample to predict from the test set
audioX_to_predict = audioX_test[100]
audioy_to_predict = audioy_test[100]

audioX_to_predict.shape

print("Real Genre:", audioy_to_predict)

# add a dimension to input data for sample - model.predict() expects a 4d array in this case
audioX_to_predict = audioX_to_predict[np.newaxis, ...] # array shape (1, 130, 13, 1)

audioX_to_predict.shape

# perform prediction
prediction = model_cnn.predict(audioX_to_predict)

# get index with max value
predicted_index = np.argmax(prediction, axis=1)

print("Predicted Genre:", int(predicted_index))

# pick a sample to predict from the test set
audioX_to_predict = audioX_test[50]
audioy_to_predict = audioy_test[50]

print("Real Genre:", audioy_to_predict)

audioX_to_predict = audioX_to_predict[np.newaxis, ...]

prediction = model_cnn.predict(audioX_to_predict)

# get index with max value
predicted_index = np.argmax(prediction, axis=1)

print("Predicted Genre:", int(predicted_index))

len(audioX_test)

for n in range(10):

  i = random.randint(0,len(audioX_test))
  # pick a sample to predict from the test set
  audioX_to_predict = audioX_test[i]
  audioy_to_predict = audioy_test[i]

  print("\nReal Genre:", audioy_to_predict)

  audioX_to_predict = audioX_to_predict[np.newaxis, ...]

  prediction = model_cnn.predict(audioX_to_predict)

  # get index with max value
  predicted_index = np.argmax(prediction, axis=1)

  print("Predicted Genre:", int(predicted_index))

pred = []
for i in range(len(audioX_test)):

  # i = random.randint(0,len(X_test))
  # pick a sample to predict from the test set
  audioX_to_predict = audioX_test[i]
  audioy_to_predict = audioy_test[i]

  # print("\nReal Genre:", y_to_predict)

  audioX_to_predict = audioX_to_predict[np.newaxis, ...]

  prediction = model_cnn.predict(audioX_to_predict)

  # get index with max value
  predicted_index = np.argmax(prediction, axis=1)

  # print("Predicted Genre:", int(predicted_index))
  pred.append(predicted_index)

"""CONFUSION MATRIX OF CNN MODEL"""

cm = confusion_matrix(audioy_test, pred)
print(cm)

import matplotlib.pyplot as plt
import numpy as np
import itertools

def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=False):
    """
    arguments
    ---------
    cm:           confusion matrix from sklearn.metrics.confusion_matrix

    target_names: given classification classes such as [0, 1, 2]
                  the class names, for example: ['high', 'medium', 'low']

    title:        the text to display at the top of the matrix

    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm
                  see http://matplotlib.org/examples/color/colormaps_reference.html

    normalize:    If False, plot the raw numbers
                  If True, plot the proportions
    """

    if cmap is None:
        cmap = plt.get_cmap('Oranges')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylim(len(target_names)-0.5, -0.5)
    plt.ylabel('True labels')
    plt.xlabel('Predicted labels')
    plt.savefig(title + '.png', dpi=500, bbox_inches = 'tight')
    plt.show()



# a tuple for all the class names
target_names = ('country', 'hiphop', 'jazz', 'metal','reggae')
plot_confusion_matrix(cm, target_names)

"""F1 SCORE"""

from sklearn.metrics import f1_score
print("F1 score of CNN model = ", f1_score(audioy_test, pred, average='weighted', sample_weight=None)*100)

"""PRECISION AND RECALL"""

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

print("Precision = ", precision_score(audioy_test, pred, average='weighted'))
print("Recall = ", precision_score(audioy_test, pred, average='weighted'))

"""Predict on a song"""

# Audio files pre-processing
def process_input(audio_file, track_duration):

  SAMPLE_RATE = 22050
  NUM_MFCC = 13
  N_FTT=2048
  HOP_LENGTH=512
  TRACK_DURATION = track_duration # measured in seconds
  SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION
  NUM_SEGMENTS = 10

  samples_per_segment = int(SAMPLES_PER_TRACK / NUM_SEGMENTS)
  num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / HOP_LENGTH)

  signal, sample_rate = librosa.load(audio_file, sr=SAMPLE_RATE)

  for d in range(10):

    # calculate start and finish sample for current segment
    start = samples_per_segment * d
    finish = start + samples_per_segment

    # extract mfcc
    mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=NUM_MFCC, n_fft=N_FTT, hop_length=HOP_LENGTH)
    mfcc = mfcc.T

    return mfcc

genre_dict = {0:"Country",1:"HipHop",2:"Jazz",3:"Metal",4:"Reggae"}

new_input_mfcc = process_input("/content/drive/MyDrive/Test/hiphop.wav", 30)

type(new_input_mfcc)

new_input_mfcc.shape

audioX_to_predict = new_input_mfcc[np.newaxis, ..., np.newaxis]
audioX_to_predict.shape

prediction = model_cnn.predict(audioX_to_predict)

np.set_printoptions(suppress=True)
np.round(prediction,3)
print(prediction[0:4])
predicted_index = np.argmax(prediction, axis=1)

print("Predicted Genre:", genre_dict[int(predicted_index)])

"""# **Image**"""

import tensorflow as tf
import os
print('done')

# Avoid OOM errors by setting GPU Memory Consumption Growth
#gpus = tf.config.experimental.list_physical_devices('GPU')
#for gpu in gpus:
#    tf.config.experimental.set_memory_growth(gpu, True)
#print('done')+

tf.config.list_physical_devices('GPU')
print('done')

import cv2
import imghdr
print('done')

from google.colab import drive
drive.mount('/content/drive')

data_dir = '/content/drive/MyDrive/google_data/'

image_exts = ['jpeg','jpg', 'bmp', 'png']

for image_class in os.listdir(data_dir):
    for image in os.listdir(os.path.join(data_dir, image_class)):
        image_path = os.path.join(data_dir, image_class, image)
        try:
            img = cv2.imread(image_path)
            tip = imghdr.what(image_path)
            if tip not in image_exts:
                print('Image not in ext list {}'.format(image_path))
                os.remove(image_path)
        except Exception as e:
            print('Issue with image {}'.format(image_path))
            # os.remove(image_path)
print('done')

import numpy as np
from matplotlib import pyplot as plt

data = tf.keras.utils.image_dataset_from_directory('/content/drive/MyDrive/google_data/')

data_iterator = data.as_numpy_iterator()

batch = data_iterator.next()

fig, ax = plt.subplots(ncols=4, figsize=(20,20))
for idx, img in enumerate(batch[0][:4]):
    ax[idx].imshow(img.astype(int))
    ax[idx].title.set_text(batch[1][idx])

data = data.map(lambda x,y: (x/255, y))

data.as_numpy_iterator().next()

train_size = int(len(data)*.7)
val_size = int(len(data)*.2)
test_size = int(len(data)*.1)

train = data.take(train_size)
val = data.skip(train_size).take(val_size)
test = data.skip(train_size+val_size).take(test_size)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout

model_sequential = Sequential()

model_sequential.add(Conv2D(64, (3,3), 1, activation='relu', input_shape=(256,256,3)))
model_sequential.add(MaxPooling2D())
model_sequential.add(Conv2D(32, (3,3), 1, activation='relu'))
model_sequential.add(MaxPooling2D())
model_sequential.add(Dropout(0.4))
model_sequential.add(Conv2D(16, (3,3), 1, activation='relu'))
model_sequential.add(MaxPooling2D())
model_sequential.add(Flatten())
model_sequential.add(Dense(256, activation='relu'))
model_sequential.add(Dense(5, activation='softmax'))

model_sequential.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

model_sequential.summary()

logdir='/content/drive/MyDrive/Capstone/Image-modality/logs'

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

#Training
hist = model_sequential.fit(train,
                 epochs=28,
                 validation_data=val,
                 callbacks=[tensorboard_callback])

fig = plt.figure()
plt.plot(hist.history['loss'], color='teal', label='loss')
plt.plot(hist.history['val_loss'], color='orange', label='val_loss')
fig.suptitle('Loss', fontsize=20)
plt.legend(loc="upper left")

plt.savefig("Image_Loss_Graph1.pdf",format="pdf")
plt.show()

fig = plt.figure()
plt.plot(hist.history['accuracy'], color='teal', label='accuracy')
plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')
fig.suptitle('Accuracy', fontsize=20)
plt.legend(loc="upper left")

plt.savefig("Image_Accuracy_Graph2.pdf",format="pdf")
plt.show()

model_sequential.save("Image.hdf5")

import cv2
img = cv2.imread('/content/drive/MyDrive/mop.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(img)
plt.show()

resize = tf.image.resize(img, (256,256))
plt.imshow(resize.numpy().astype(int))
plt.show()

genre_dict = {0:"Country",1:"HipHop",2:"Jazz",3:"Metal",4:"Reggae"}

hist = model_sequential.predict(np.expand_dims(resize/255, 0))
hist
predicted_index = np.argmax(hist, axis=1)

print("Predicted Genre:", genre_dict[int(predicted_index)])

"""# **Lryrics**"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import sys
import json
import os
import re
import random as r

# This is what we are using for data preparation and ML part
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score
# Different ML models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.svm import NuSVC
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import AdaBoostClassifier
#Global parameters
SONGS_PER_GENRE = 10000
SONGS_PER_TRAINING = 1000
SONGS_PER_TESTING = 100

#we need to cleanse lyrics to remove special characters and garbage
#this function cleanses single string
def cleanse (text):
    result = re.sub('[^a-zA-Z0-9 ]', ' ', text)
    return result

#Country
partists_cou = pd.read_csv('/content/drive/MyDrive/Capstone/Text-modality/artists-data_cou.csv') #load the list of artists
psongs_cou = pd.read_csv('/content/drive/MyDrive/Capstone/Text-modality/lyrics-data.csv') #load the list of songs

cou_artists = partists_cou[partists_cou['Genres']=='Country'] # filter artists by genre
cou_songs = pd.merge(psongs_cou, cou_artists, how='inner', left_on='ALink', right_on='Link') #inner join of pop artists with songs to get only songs by pop artists
cou_songs = cou_songs[['Genres', 'Artist', 'SName', 'Lyric']].rename(columns={'SName':'Song'})#leave only columns of interest and rename some of them.
cou_songs = cou_songs.dropna() # Remove incomplete records, cleanse lyrics
cou_songs = cou_songs[cou_songs['Lyric']!='Instrumental'].head(SONGS_PER_GENRE).applymap(cleanse) #Remove instrumental compositions  and limit the size of final dataset
cou_songs.shape

#Hip Hop
partists_hiphop = pd.read_csv('/content/drive/MyDrive/Capstone/Text-modality/hiphop_text.csv') #load the list of artists
psongs_hiphop = pd.read_csv('/content/drive/MyDrive/Capstone/Text-modality/lyrics-data.csv') #load the list of songs

hiphop_artists = partists_hiphop[partists_hiphop['Genres']=='Hip Hop'] # filter artists by genre
hiphop_songs = pd.merge(psongs_hiphop, hiphop_artists, how='inner', left_on='ALink', right_on='Link') #inner join of pop artists with songs to get only songs by pop artists
hiphop_songs = hiphop_songs[['Genres', 'Artist', 'SName', 'Lyric']].rename(columns={'SName':'Song'})#leave only columns of interest and rename some of them.
hiphop_songs = hiphop_songs.dropna() # Remove incomplete records, cleanse lyrics
hiphop_songs = hiphop_songs[hiphop_songs['Lyric']!='Instrumental'].head(SONGS_PER_GENRE).applymap(cleanse) #Remove instrumental compositions  and limit the size of final dataset
hiphop_songs.head()

#Jazz
partists_jazz = pd.read_csv('/content/drive/MyDrive/Capstone/Text-modality/jazz_text.csv') #load the list of artists
psongs_jazz = pd.read_csv('/content/drive/MyDrive/Capstone/Text-modality/lyrics-data.csv') #load the list of songs

jazz_artists = partists_jazz[partists_jazz['Genres']=='Jazz'] # filter artists by genre
jazz_songs = pd.merge(psongs_jazz, jazz_artists, how='inner', left_on='ALink', right_on='Link') #inner join of pop artists with songs to get only songs by pop artists
jazz_songs = jazz_songs[['Genres', 'Artist', 'SName', 'Lyric']].rename(columns={'SName':'Song'})#leave only columns of interest and rename some of them.
jazz_songs = jazz_songs.dropna() # Remove incomplete records, cleanse lyrics
jazz_songs = jazz_songs[jazz_songs['Lyric']!='Instrumental'].head(SONGS_PER_GENRE).applymap(cleanse) #Remove instrumental compositions  and limit the size of final dataset
jazz_songs.head()

#Metal
partists_metal = pd.read_csv('/content/drive/MyDrive/Capstone/Text-modality/metal_text.csv') #load the list of artists
psongs_metal = pd.read_csv('/content/drive/MyDrive/Capstone/Text-modality/lyrics-data.csv') #load the list of songs

metal_artists = partists_metal[partists_metal['Genres']=='Metal'] # filter artists by genre
metal_songs = pd.merge(psongs_metal, metal_artists, how='inner', left_on='ALink', right_on='Link') #inner join of pop artists with songs to get only songs by pop artists
metal_songs = metal_songs[['Genres', 'Artist', 'SName', 'Lyric']].rename(columns={'SName':'Song'})#leave only columns of interest and rename some of them.
metal_songs = metal_songs.dropna() # Remove incomplete records, cleanse lyrics
metal_songs = metal_songs[metal_songs['Lyric']!='Instrumental'].head(SONGS_PER_GENRE).applymap(cleanse) #Remove instrumental compositions  and limit the size of final dataset
metal_songs.head()

#Reggae
partists_reggae = pd.read_csv('/content/drive/MyDrive/Capstone/Text-modality/reggae_text.csv') #load the list of artists
psongs_reggae = pd.read_csv('/content/drive/MyDrive/Capstone/Text-modality/lyrics-data.csv') #load the list of songs

reggae_artists = partists_reggae[partists_reggae['Genres']=='Reggae'] # filter artists by genre
reggae_songs = pd.merge(psongs_reggae, reggae_artists, how='inner', left_on='ALink', right_on='Link') #inner join of pop artists with songs to get only songs by pop artists
reggae_songs = reggae_songs[['Genres', 'Artist', 'SName', 'Lyric']].rename(columns={'SName':'Song'})#leave only columns of interest and rename some of them.
reggae_songs = reggae_songs.dropna() # Remove incomplete records, cleanse lyrics
reggae_songs = reggae_songs[reggae_songs['Lyric']!='Instrumental'].head(SONGS_PER_GENRE).applymap(cleanse) #Remove instrumental compositions  and limit the size of final dataset
reggae_songs.head()

print("Reggae -",len(reggae_songs), "| Hiphop -", len(hiphop_songs), "| Metal -",len(metal_songs), "| Country" , len(cou_songs), 'Jazz | ' ,len(jazz_songs))

training_data = pd.concat([cou_songs.head(SONGS_PER_TRAINING), hiphop_songs.head(SONGS_PER_TRAINING),  jazz_songs.head(SONGS_PER_TRAINING),  metal_songs.head(SONGS_PER_TRAINING),reggae_songs.head(SONGS_PER_TRAINING)])
training_data.shape

!pip install transformers

!pip install lime

## for data
import json
import pandas as pd
import numpy as np
## for plotting
import matplotlib.pyplot as plt
import seaborn as sns
## for processing
import re
import nltk
## for bag-of-words
from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing
## for explainer
from lime import lime_text
## for word embedding
import gensim
import gensim.downloader as gensim_api
## for deep learning
from tensorflow.keras import models, layers, preprocessing as kprocessing
from tensorflow.keras import backend as K
## for bert language model
import transformers

def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):
    ## clean (convert to lowercase and remove punctuations and
    #characters and then strip)
    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())

    ## Tokenize (convert from string to list)
    lst_text = text.split()
    ## remove Stopwords
    if lst_stopwords is not None:
        lst_text = [word for word in lst_text if word not in
                    lst_stopwords]

    ## Stemming (remove -ing, -ly, ...)
    if flg_stemm == True:
        ps = nltk.stem.porter.PorterStemmer()
        lst_text = [ps.stem(word) for word in lst_text]

    ## Lemmatisation (convert the word into root word)
    if flg_lemm == True:
        lem = nltk.stem.wordnet.WordNetLemmatizer()
        lst_text = [lem.lemmatize(word) for word in lst_text]

    ## back to string from list
    text = " ".join(lst_text)
    return text

import nltk
nltk.download("stopwords")
lst_stopwords = nltk.corpus.stopwords.words("english")
lst_stopwords

print(type(training_data["Lyric"]))

import nltk
nltk.download('all')

training_data["LyricClean"] = training_data["Lyric"].apply(lambda x:
          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True,
          lst_stopwords=lst_stopwords))
training_data.head()

## split dataset
import sklearn
dtf_train,dtf_test, y_train, y_test = sklearn.model_selection.train_test_split(training_data, training_data['Genres'], test_size=0.25)
#dtf_train, dtf_test = model_selection.train_test_split(training_data, test_size=0.3)
## get target

vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))

corpus = dtf_train["LyricClean"]
vectorizer.fit(corpus)
X_train = vectorizer.transform(corpus)
dic_vocabulary = vectorizer.vocabulary_

from sklearn import feature_selection
y = dtf_train["Genres"]
X_names = vectorizer.get_feature_names_out()
print(X_names)
p_value_limit = 0.95
dtf_features = pd.DataFrame()
for cat in np.unique(y):
    chi2, p = feature_selection.chi2(X_train, y==cat)
    dtf_features = dtf_features.append(pd.DataFrame(
                   {"feature":X_names, "score":1-p, "y":cat}))
    dtf_features = dtf_features.sort_values(["y","score"],
                    ascending=[True,False])
    dtf_features = dtf_features[dtf_features["score"]>p_value_limit]
X_names = dtf_features["feature"].unique().tolist()

for cat in np.unique(y):
   print("# {}:".format(cat))
   print("  . selected features:",
         len(dtf_features[dtf_features["y"]==cat]))
   print("  . top features:", ",".join(
dtf_features[dtf_features["y"]==cat]["feature"].values[:10]))
   print(" ")

vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)
vectorizer.fit(corpus)
X_train = vectorizer.transform(corpus)
dic_vocabulary = vectorizer.vocabulary_

classifier = LogisticRegression(multi_class='multinomial',max_iter=1000, solver='lbfgs')

## pipeline
model = pipeline.Pipeline([("vectorizer", vectorizer),
                           ("classifier", classifier)])
## train classifier
model["classifier"].fit(X_train, y_train)
## test
X_test = dtf_test["LyricClean"].values
predicted = model.predict(X_test)
predicted_prob = model.predict_proba(X_test)

user_input = input()
test = user_input.lower()
final_test = utils_preprocess_text(user_input, flg_stemm=False, flg_lemm=True, lst_stopwords=None)
print(final_test)

predicted_test = model.predict([final_test])
print(predicted)

predicted_prob_test = model.predict_proba([final_test])
print(predicted_prob)

y_test

from sklearn import metrics
classes = np.unique(y_test)
y_test_array = pd.get_dummies(y_test, drop_first=False).values

## Accuracy, Precision, Recall
accuracy = metrics.accuracy_score(y_test, predicted)
auc = metrics.roc_auc_score(y_test, predicted_prob,multi_class="ovr")
print("Accuracy:",  round(accuracy,2))
print("Auc:", round(auc,2))
print("Detail:")
print(metrics.classification_report(y_test, predicted))

## Plot confusion matrix
cm = metrics.confusion_matrix(y_test, predicted)
fig, ax = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues,
            cbar=False)
ax.set(xlabel="Pred", ylabel="True", xticklabels=classes,
       yticklabels=classes, title="Confusion matrix")
plt.yticks(rotation=0)

fig, ax = plt.subplots(nrows=1, ncols=2)
## Plot roc
for i in range(len(classes)):
    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],
                           predicted_prob[:,i])
    ax[0].plot(fpr, tpr, lw=3,
              label='{0} (area={1:0.2f})'.format(classes[i],
                              metrics.auc(fpr, tpr))
               )
ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')
ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05],
          xlabel='False Positive Rate',
          ylabel="True Positive Rate (Recall)",
          title="Receiver operating characteristic")
ax[0].legend(loc="lower right")
ax[0].grid(True)

## Plot precision-recall curve
for i in range(len(classes)):
    precision, recall, thresholds = metrics.precision_recall_curve(
                 y_test_array[:,i], predicted_prob[:,i])
    ax[1].plot(recall, precision, lw=3,
               label='{0} (area={1:0.2f})'.format(classes[i],
                                  metrics.auc(recall, precision))
              )
ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',
          ylabel="Precision", title="Precision-Recall curve")
ax[1].legend(loc="best")
ax[1].grid(True)
plt.show()

"""# **Ensemble**"""

from keras.models import load_model
from sklearn.metrics import accuracy_score

"""Audio"""

new_input_mfcc = process_input("/content/drive/MyDrive/Test/hiphop.wav", 30)
type(new_input_mfcc)
new_input_mfcc.shape
audioX_to_predict = new_input_mfcc[np.newaxis, ..., np.newaxis]
audioX_to_predict.shape



audiopred = model_cnn.predict(audioX_to_predict)

np.set_printoptions(suppress=True)

genre_dict = {0:"Country",1:"HipHop",2:"Jazz",3:"Metal",4:"Reggae"}
audiopred = audiopred[0,:5]
print(audiopred)
#predicted_index = np.argmax(prediction, axis=1)
predicted_index1 = np.argmax(audiopred)
#print("Predicted Genre:", genre_dict[int(predicted_index1)])

"""Image"""

import cv2
img = cv2.imread('/content/drive/MyDrive/Test/hiphop.png')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(img)
plt.show()

resize = tf.image.resize(img, (256,256))
plt.imshow(resize.numpy().astype(int))
plt.show()

hist = model_sequential.predict(np.expand_dims(resize/255, 0))
imagepred = hist[0,:5]
print(imagepred)

"""Lyrics"""

user_input = input()
test = user_input.lower()
final_test = utils_preprocess_text(user_input, flg_stemm=False, flg_lemm=True, lst_stopwords=None)
print(final_test)

predicted = model.predict([final_test])
print(predicted)

predicted_prob = model.predict_proba([final_test])
print(predicted_prob)
lyricspred=predicted_prob[0]
print(lyricspred)

genre_dict = {0:"Country",1:"HipHop",2:"Jazz",3:"Metal",4:"Reggae"}

summed = imagepred + audiopred + lyricspred
den = 0
for i in summed:
    den += (i*i)
den = den ** 0.5
for i in range(len(summed)):
    summed[i] = summed[i] / den
print(summed)

print("Image modality:", imagepred)
print("Audio modality:", audiopred)
print("Lyrics modality:", lyricspred)
print("Ensemble of models:",  summed)
predicted_index_e = np.argmax(summed)

print("Predicted Genre:", genre_dict[int(predicted_index_e)])